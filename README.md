# scraper
a. URL of the Report.
https://github.com/BtlAyc/scraper

b. Used tech-stack(library/framework etc.)
openpyxl to process xlsx, BeautifulSoap and lxml parser for web content scraping, and google services to write google sheets

c. A brief description of the challenges you face. 
I have to learn new things like Google Apps Script quickly, because of time restriction.

d. What did you learn from this project?
I knew what web scraping is but as term. I can apply codes with this project.

e. Answers of the additional questions

1. If I’d have 10.000 urls that I should visit, then it takes hours to finish. What can we make to fasten this process?
    In my opinion, the thing that takes time the most is deciding which pages are product page. If there would be product tags top of the html body, maybe it can be faster. Or maybe can use another parsing method.

2. What can we make or use to automate this process to run once a day? Write your recommendations.
    I didn't have a knowledge about that sorry. But I quikly searched it and see there is some python libraries such as schedule. I would use that.

3. Please briefly explain what an API is and how it works.
    Api's allow to communicate two seperate application.A client application initiates an API call to retrieve information—also known as a request. This request is processed from an application to the web server via the API’s Uniform Resource Identifier (URI) and includes a request verb, headers, and sometimes, a request body.
After receiving a valid request, the API makes a call to the external program or web server.
The server sends a response to the API with the requested information.
The API transfers the data to the initial requesting application